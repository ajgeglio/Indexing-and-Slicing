{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ageglio\\AppData\\Local\\Temp\\1\\ipykernel_9156\\3946088740.py:1: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_unp = pd.read_csv('all_unpacked_images_metadata.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599180, 19)\n"
     ]
    }
   ],
   "source": [
    "df_unp = pd.read_csv('all_unpacked_images_metadata.csv', index_col=0)\n",
    "print(df_unp.shape) # (599180, 19)\n",
    "## Getting indexes of Camera system, lens, etc\n",
    "is2019 = (df_unp['Datetime'] > \"2018-12-31\") & (df_unp['Datetime'] <= \"2019-12-31\")\n",
    "is2020 = (df_unp['Datetime'] > \"2019-12-31\") & (df_unp['Datetime'] <= \"2020-12-31\")\n",
    "is2021 = (df_unp['Datetime'] > \"2020-12-31\") & (df_unp['Datetime'] <= \"2021-12-31\")\n",
    "is2022 = (df_unp['Datetime'] > \"2021-12-31\") & (df_unp['Datetime'] <= \"2022-12-31\")\n",
    "is2023 = (df_unp['Datetime'] > \"2022-12-31\") & (df_unp['Datetime'] <= \"2023-12-31\")\n",
    "\n",
    "## for Iver3069\n",
    "is2021_befor0512 = (df_unp['Datetime'] > \"2020-12-31\") & (df_unp['Datetime'] <= \"2021-05-11\")\n",
    "is2021_after0512 = (df_unp['Datetime'] > \"2021-05-11\") & (df_unp['Datetime'] <= \"2021-12-31\")\n",
    "is2022_prior0716 = (df_unp['Datetime'] > \"2021-12-31\") & (df_unp['Datetime'] <= \"2022-07-15\")\n",
    "is2022_0720t0809 = (df_unp['Datetime'] > \"2022-07-19\") & (df_unp['Datetime'] <= \"2022-08-09\")\n",
    "is2022_0810t0814 = (df_unp['Datetime'] > \"2022-08-09\") & (df_unp['Datetime'] <= \"2022-08-14\")\n",
    "is2022_0815t0818 = (df_unp['Datetime'] > \"2022-08-14\") & (df_unp['Datetime'] <= \"2022-08-18\")\n",
    "is2022_0819t0822 = (df_unp['Datetime'] > \"2022-08-18\") & (df_unp['Datetime'] <= \"2022-08-22\")\n",
    "is2022_after0822 = (df_unp['Datetime'] > \"2022-08-22\") & (df_unp['Datetime'] <= \"2022-12-31\")\n",
    "\n",
    "is2023_after0809 = (df_unp['Datetime'] > \"2023-08-08\") & (df_unp['Datetime'] <= \"2022-12-31\")\n",
    "\n",
    "## for Iver3098\n",
    "is2022_0705t0810 = (df_unp['Datetime'] > \"2022-07-05\") & (df_unp['Datetime'] <= \"2022-08-10\")\n",
    "is2022_0811t0819 = (df_unp['Datetime'] > \"2022-08-10\") & (df_unp['Datetime'] <= \"2022-08-19\")\n",
    "is2022_after0819 = (df_unp['Datetime'] > \"2022-08-19\") & (df_unp['Datetime'] <= \"2022-12-31\")\n",
    "\n",
    "is2023_0427t0522 = (df_unp['Datetime'] > \"2023-04-27\") & (df_unp['Datetime'] <= \"2023-05-22\")\n",
    "is2023_0523t0711 = (df_unp['Datetime'] > \"2023-05-22\") & (df_unp['Datetime'] <= \"2023-07-11\")\n",
    "is2023_0712t0808 = (df_unp['Datetime'] > \"2023-07-11\") & (df_unp['Datetime'] <= \"2023-08-08\")\n",
    "is2023_after0808 = (df_unp['Datetime'] > \"2023-08-08\") & (df_unp['Datetime'] <= \"2023-12-31\")\n",
    "\n",
    "is3069 = (df_unp['AUV']==\"Iver3069\")\n",
    "is3098 = (df_unp['AUV']==\"Iver3098\")\n",
    "isABS1 = (df_unp['cam_sys']==\"ABS1\")\n",
    "isABS2 = (df_unp['cam_sys']==\"ABS2\")\n",
    "\n",
    "### major groupings ###\n",
    "## 2019-2020 ABS1 3069 \n",
    "group_01  = df_unp[is2019 & isABS1 & is3069].index\n",
    "group_02 =  df_unp[is2020 & isABS1 & is3069].index\n",
    "\n",
    "## 2021 ABS1 3069 \n",
    "group_03 =  df_unp[is2021_befor0512 & isABS1 & is3069].index\n",
    "group_04 =  df_unp[is2021_after0512 & isABS1 & is3069].index\n",
    "\n",
    "## 2022 ABS1 3069 \n",
    "group_05 =  df_unp[is2022_prior0716 & isABS1 & is3069].index\n",
    "\n",
    "## 2022 ABS2 3069 \n",
    "group_06 =  df_unp[is2022_0720t0809 & isABS2 & is3069].index\n",
    "group_07 =  df_unp[(is2022_0810t0814 | is2022_0819t0822) & isABS2 & is3069].index\n",
    "group_08 =  df_unp[is2022_0815t0818 & isABS2 & is3069].index\n",
    "group_09 =  df_unp[is2022_after0822 & isABS2 & is3069].index\n",
    "\n",
    "## 2022 ABS2 3098 \n",
    "group_10 =  df_unp[is2022_0705t0810 & isABS2 & is3098].index\n",
    "group_11 =  df_unp[is2022_0811t0819 & isABS2 & is3098].index\n",
    "group_12 =  df_unp[is2022_after0819 & isABS2 & is3098].index\n",
    "\n",
    "## 2023 ABS2 3098 \n",
    "group_13 =  df_unp[is2023_0427t0522 & isABS2 & is3098].index\n",
    "group_14 =  df_unp[is2023_0523t0711 & isABS2 & is3098].index\n",
    "group_15 =  df_unp[is2023_0712t0808 & isABS2 & is3098].index\n",
    "\n",
    "## 2023 ABS2 3069 \n",
    "group_16 =  df_unp[is2023_after0808 & isABS2 & is3069].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking for missed idx\n",
    "\n",
    "all_groups = group_01.union(group_02).union(group_03).union(group_04).union(\n",
    "             group_05).union(group_06).union(group_07).union(group_08).union(\n",
    "             group_09).union(group_10).union(group_11).union(group_12).union(\n",
    "             group_13).union(group_14).union(group_15)\n",
    "df_unp.index.difference(all_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3363, 193971, 3855, 174036, 74222, 10446, 3330, 769, 38791, 27144, 9417, 17852, 1785, 19168, 21031, 0, total 599180\n"
     ]
    }
   ],
   "source": [
    "groups = [group_01, group_02, group_03, group_04, group_05, \n",
    "          group_06, group_07, group_08, group_09, group_10, \n",
    "          group_11, group_12, group_13, group_14, group_15, group_16]\n",
    "i = 0\n",
    "for group in groups:\n",
    "    l = len(group)\n",
    "    print(l, end=\", \")\n",
    "    i +=l\n",
    "print(\"total\", i)\n",
    "# 3363, 193971, 3855, 174036, 74222, 10446, 3330, 769, 38791, 27144, 9417, 17852, 1785, 19168, 21031, 0, total 599180\n",
    "# 3363, 193971, 3855, 174036, 74222, 10446, 3330, 769, 38791, 27144, 9417, 17852, 1785, 19168, 10097, 0, total 588246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying 1785 / 1785  \r"
     ]
    }
   ],
   "source": [
    "def del_files(filepath_list): \n",
    "    for filepath in filepath_list:\n",
    "        if os.path.isfile(filepath):\n",
    "            os.remove(filepath)\n",
    "        else:\n",
    "            # If it fails, inform the user.\n",
    "            print(\"Error: %s file not found\" % filepath)\n",
    "def copy_imgs_2_drive(df, group, dest_folder):\n",
    "    img_pth_list = list(df.loc[group, 'image_path'])\n",
    "    img_nam_list = list(df.loc[group, 'image_name'])\n",
    "    num_imgs = len(img_nam_list)\n",
    "    i=0\n",
    "    for img_pth, img_name in zip(img_pth_list, img_nam_list):\n",
    "        src = img_pth\n",
    "        dest = os.path.join(dest_folder, img_name)\n",
    "        # File copy was interrupted often due to network, added src/dest comparison\n",
    "        if os.path.exists(dest):\n",
    "            if os.stat(src).st_size == os.stat(dest).st_size:\n",
    "                i+=1\n",
    "            else:\n",
    "                shutil.copy(src, dest)\n",
    "                i+=1\n",
    "        else:\n",
    "            shutil.copy(src, dest)\n",
    "            i+=1\n",
    "        print(\"Copying\", i,\"/\",num_imgs, end='  \\r')\n",
    "## Sea Lamprey\n",
    "## Groups: 4-, 10- ,11-, 12-, 13, 14-, 15-\n",
    "\n",
    "## Lake Chub\n",
    "## Groups 01-, 02-, 03-, 05-, 06-, 07-, 08-, 09-\n",
    "group = group_13\n",
    "dest_folder = \"G:\\\\group_13\"\n",
    "\n",
    "copy_imgs_2_drive(df_unp, group, dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Images are typically 9-11 Mb, so one way to filter out unusable images is to set a threshold for file-size\n",
    "Here, I set the threshold to 4 Mb, and I found there were 3,334 images in group 4 alone below 4 mb, probably unusable\n",
    "'''\n",
    "src_folder = glob.glob(r\"G:\\group_04\\*\")\n",
    "thresh = 4e6\n",
    "def select_imgs_by_size(src_folder, dest_folder=None, thresh=4e5):\n",
    "    sm_files_list = []\n",
    "    for file in src_folder:\n",
    "        s = os.path.getsize(file)\n",
    "        if s < thresh:\n",
    "            sm_files_list.append(s)\n",
    "            # shutil.move(file, dest_folder) ## If you want to move them\n",
    "    return sm_files_list\n",
    "\n",
    "len(select_imgs_by_size(src_folder))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubbler-lite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
